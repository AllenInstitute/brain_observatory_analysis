{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "494261ad-3712-45d9-bac2-cfd4e45a0e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iryna.yavorska\\Anaconda3\\envs\\mFish_glm\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "\n",
    "#processing\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy import signal\n",
    "from brain_observatory_analysis.behavior.video_qc import annotation_tools\n",
    "from brain_observatory_qc.data_access import from_lims_utilities\n",
    "from brain_observatory_analysis.ophys.experiment_loading import start_lamf_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b027a3d-e737-47e8-b9dc-35979ff0a7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b08cf7-c02b-43fa-8d9b-d9297080f1ee",
   "metadata": {},
   "source": [
    "#### load file with neuropixel paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4caefad-5403-4070-b628-29a37c0870e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['experiment_id', 'face_video', 'face_video_metadata', 'face_dlc_output',\n",
       "       'side_video', 'side_video_metadata', 'side_dlc_output', 'eye_video',\n",
       "       'eye_video_metadata', 'eye_dlc_output'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'vbn_video_paths.xlsx'\n",
    "vbn_video_paths = pd.read_excel(filename)\n",
    "vbn_video_paths.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68fc9e1d-c1c8-4b05-b755-3e3e9e125869",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This table connects specimen ids in prod0 folder to mouse ids.\n",
    "### Clark will add a function to brain_observatory_qc that can do that without this table\n",
    "base_dir = '//allen/programs/mindscope/workgroups/learning/behavioral_video_plots'\n",
    "mouse_table = pd.read_csv(os.path.join(base_dir, '20230713_SS_LAMFdirectories.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7439f642-04b6-4455-94c2-034c214e9dbf",
   "metadata": {},
   "source": [
    "#### learning mFish ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4f6c2dd-fc1d-49d7-84db-67e95dd148b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be on older version of allensdk branch, let MJD know if not\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\iryna.yavorska\\documents\\github\\allensdk_mjd\\allensdk\\allensdk\\brain_observatory\\behavior\\behavior_project_cache\\tables\\util\\prior_exposure_processing.py:165: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  counts = df.groupby(['mouse_id'])['to'].apply(cumsum)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1142290477, 1148010756, 1155072208, 1172320751, 1179706234,\n",
       "       1187453999, 1194433355, 1199272999, 1216869852], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_table = start_lamf_analysis()\n",
    "np.unique(experiment_table.donor_id.apply(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0972439-bdb1-4e92-961a-cfcf2eed1d86",
   "metadata": {},
   "source": [
    "#### paths to example ophys videos and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9934798d-294f-439d-8e4c-6deebfdf5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = 'videos'\n",
    "\n",
    "folder = 'face_tracking'#, 'side_tracking', 'eye_tracking',\n",
    "\n",
    "input_path = 'E:/face_video_annotation-iryna-2023-08-28/videos'\n",
    "\n",
    "dlc_path = 'E:/face_video_annotation-iryna-2023-08-28/previous_dlc'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2be758a2-b00b-4c4b-9044-eef3e14c3979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 6 h5 files\n"
     ]
    }
   ],
   "source": [
    "dlc_files = annotation_tools.get_h5file(dlc_path )\n",
    "dlc_files = np.sort(dlc_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fd6410c8-ef57-4c3f-bddd-831e1b7d79ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path =  '//allen/programs/mindscope/workgroups/learning/behavioral_video_annotation/NCB_plots/face'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1f376d98-87b3-42b7-9edd-aa24c5475610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded DLC data\n",
      "loaded DLC data\n",
      "loaded DLC data\n",
      "loaded DLC data\n",
      "loaded DLC data\n",
      "loaded DLC data\n"
     ]
    }
   ],
   "source": [
    "all_DLC = []\n",
    "for dlc_file in dlc_files:\n",
    "    all_DLC.append(annotation_tools.read_DLC_h5file(dlc_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308125ae-9ccc-4cea-9b71-91eb520acdfa",
   "metadata": {},
   "source": [
    "#### load movie and plot tracked points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a0627dcf-8b35-4876-bdc8-e812420c0a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera view was not provided, returning all mp4 files\n",
      "mp4 file: ['E:/face_video_annotation-iryna-2023-08-28/videos\\\\1170569654_Face_20220413T101321.mp4', 'E:/face_video_annotation-iryna-2023-08-28/videos\\\\1188450756_Face_20220701T090612.mp4', 'E:/face_video_annotation-iryna-2023-08-28/videos\\\\1193461885_Face_20220722T084813.mp4', 'E:/face_video_annotation-iryna-2023-08-28/videos\\\\1224566242_Face_20221108T085010.mp4', 'E:/face_video_annotation-iryna-2023-08-28/videos\\\\1226957088_Face_20221118T091700.mp4', 'E:/face_video_annotation-iryna-2023-08-28/videos\\\\1229836719_Face_20221201T090953.mp4', 'E:/face_video_annotation-iryna-2023-08-28/videos\\\\1231992811_Face_20221210T113121.mp4']\n"
     ]
    }
   ],
   "source": [
    "mp4files = annotation_tools.get_mp4file(input_path)\n",
    "mp4files = np.sort(mp4files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "571dcc77-6b6f-4647-881a-766b2b002dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['E:/face_video_annotation-iryna-2023-08-28/previous_dlc\\\\1188450756_Face_20220701T090612DeepCut_resnetNone_np3_face_camMar30shuffle1_1030000.h5',\n",
       "       'E:/face_video_annotation-iryna-2023-08-28/previous_dlc\\\\1193461885_Face_20220722T084813DeepCut_resnetNone_np3_face_camMar30shuffle1_1030000.h5',\n",
       "       'E:/face_video_annotation-iryna-2023-08-28/previous_dlc\\\\1224566242_Face_20221108T085010DeepCut_resnetNone_np3_face_camMar30shuffle1_1030000.h5',\n",
       "       'E:/face_video_annotation-iryna-2023-08-28/previous_dlc\\\\1226957088_Face_20221118T091700DeepCut_resnetNone_np3_face_camMar30shuffle1_1030000.h5',\n",
       "       'E:/face_video_annotation-iryna-2023-08-28/previous_dlc\\\\1229836719_Face_20221201T090953DeepCut_resnetNone_np3_face_camMar30shuffle1_1030000.h5',\n",
       "       'E:/face_video_annotation-iryna-2023-08-28/previous_dlc\\\\1231992811_Face_20221210T113121DeepCut_resnetNone_np3_face_camMar30shuffle1_1030000.h5'],\n",
       "      dtype='<U141')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlc_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da069aad-9fef-42fa-9499-4ff716fbea58",
   "metadata": {},
   "source": [
    "## example use of video processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a68a8b1-2a62-4e99-ac8e-f3883db27bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bodyparts = ['whisker_pad_r', 'whisker_pad_l', 'nose_tip', 'tongue',\n",
    "       'paw_forward_l', 'wrist_l', 'paw_forward_lh', 'wrist_lh',\n",
    "       'paw_forward_r', 'wrist_r', 'paw_forward_rh', 'wrist_rh',\n",
    "       'tail_mid', 'tail_base', 'spout_tip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4169d2a7-1c37-414f-a70d-6d94c8ca7b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video E:/face_video_annotation-iryna-2023-08-28/videos\\1188450756_Face_20220701T090612.mp4 with 60.0 fps at 658x492...\n",
      "found #220487 frames to plot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [06:05<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video processing complete.\n",
      "Processing video E:/face_video_annotation-iryna-2023-08-28/videos\\1193461885_Face_20220722T084813.mp4 with 60.0 fps at 658x492...\n",
      "found #273619 frames to plot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [06:10<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video processing complete.\n",
      "Processing video E:/face_video_annotation-iryna-2023-08-28/videos\\1224566242_Face_20221108T085010.mp4 with 30.0 fps at 658x492...\n",
      "found #113852 frames to plot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [06:08<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video processing complete.\n",
      "Processing video E:/face_video_annotation-iryna-2023-08-28/videos\\1226957088_Face_20221118T091700.mp4 with 30.0 fps at 658x492...\n",
      "found #110340 frames to plot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [06:26<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video processing complete.\n",
      "Processing video E:/face_video_annotation-iryna-2023-08-28/videos\\1229836719_Face_20221201T090953.mp4 with 30.0 fps at 658x492...\n",
      "found #110517 frames to plot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [06:57<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video processing complete.\n",
      "Processing video E:/face_video_annotation-iryna-2023-08-28/videos\\1231992811_Face_20221210T113121.mp4 with 30.0 fps at 658x492...\n",
      "found #113616 frames to plot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [06:24<00:00,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:2\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i,m in enumerate(mp4files):\n",
    "    dlc =  all_DLC[i]#[all_DLC[i].bodyparts.isin(bodyparts)]\n",
    "    annotation_tools.process_video(output_path, mp4files[i+1], dlc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "711679d1-6a36-4d12-84d9-257fb3c45682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([1153099568, 1153099558, 1153099564, 1153099565, 1153099562,\n",
       "            1153099555, 1153099561, 1153099559, 1153662768, 1153662777,\n",
       "            ...\n",
       "            1231598631, 1231598628, 1232481420, 1232481421, 1232481433,\n",
       "            1232481418, 1232481426, 1232481428, 1232481429, 1232481423],\n",
       "           dtype='int64', name='ophys_experiment_id', length=1160)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_table.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54451df3-2df1-4610-81bd-c48a70e8f035",
   "metadata": {},
   "source": [
    "#### get_general_info_for_LIMS_imaging_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4103f73-efe7-4643-8aec-210c7e29b900",
   "metadata": {},
   "outputs": [],
   "source": [
    "ophys_experiment_id = 1153099561\n",
    "general_info = from_lims_utilities.get_general_info_for_LIMS_imaging_id(\"ophys_experiment_id\", ophys_experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3812dd0-e734-48a5-a0e1-4655db20682f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ophys_experiment_id', 'ophys_session_id', 'behavior_session_id',\n",
       "       'foraging_id', 'ophys_container_id', 'supercontainer_id',\n",
       "       'experiment_workflow_state', 'session_workflow_state',\n",
       "       'container_workflow_state', 'specimen_id', 'donor_id', 'specimen_name',\n",
       "       'date_of_acquisition', 'session_type', 'targeted_structure', 'depth',\n",
       "       'equipment_name', 'project', 'experiment_storage_directory',\n",
       "       'behavior_storage_directory', 'session_storage_directory',\n",
       "       'container_storage_directory', 'supercontainer_storage_directory',\n",
       "       'specimen_storage_directory'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_info.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e833db30-f8b3-4c29-bdfc-d927531dc0ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## example use of frame selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "77b814e6-cc23-4b26-ac26-b6aec30c08ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json file: //allen/programs/mindscope/production/learning/prod0/specimen_1153540225\\ophys_session_1165583926\\1165583926_Face_20220321T092804.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam_json= annotation_tools.get_jsonfile(input_path, folder)\n",
    "type(cam_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c6a17b5e-b5ab-4066-ac17-d29e51fff1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sync file: //allen/programs/mindscope/production/learning/prod0/specimen_1153540225\\ophys_session_1165583926\\1165583926_20220321T9283.h5\n"
     ]
    }
   ],
   "source": [
    "syncfile = annotation_tools.get_syncfile(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5c4e632a-c804-4ca6-a8a1-5de7fff14ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sync dataset: <allensdk.brain_observatory.sync_dataset.Dataset object at 0x000002134C65A670>\n"
     ]
    }
   ],
   "source": [
    "sync_dataobject = annotation_tools.get_sync_dataset(syncfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "030b819b-ebbf-4e8b-a467-06686349bbe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'//allen/programs/mindscope/production/learning/prod0/specimen_1153540225\\\\ophys_session_1165583926\\\\1165583926_Face_20220321T092804.json'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5a3582ff-9583-4df8-8584-022b4e7d8aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_times = annotation_tools.get_cam_timestamps(sync_dataobject, cam_json, account_for_metadata_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "42878d40-5b6d-4475-a423-b08b4ca98ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([           nan, 3.02490000e-01, 3.19150000e-01, ...,\n",
       "       4.80008142e+03, 4.80009809e+03, 4.80011475e+03])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## \n",
    "frame_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8ef721-fcdb-406c-bfa0-52e34e458233",
   "metadata": {},
   "source": [
    "#### Use timestamps to get the same frames from a different video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "75f40662-3530-4b2f-bfaf-1a6cbb383a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'//allen/programs/mindscope/workgroups/learning/behavioral_video_plots\\\\specimen_1153540225\\\\ophys_session_1165583926\\\\face_tracking'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "45e009ac-0b60-4ef7-bc77-d9ccab3b1c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FAME FRAMEs\n",
    "face_frame_dir = r'\\\\allen\\programs\\mindscope\\workgroups\\learning\\behavioral_video_annotation\\specimen_1148010767\\ophys_session_1158851430_face-iryna-2023-07-27\\labeled-data\\1158851430_Face_20220217T111922'\n",
    "\n",
    "face_frames = glob.glob(face_frame_dir + '/img*')\n",
    "\n",
    "created_times = [os.path.getctime(f) for f in face_frames]\n",
    "\n",
    "# get frame numbers from file names\n",
    "frame_no = [annotation_tools.get_frame_index(file) for file in face_frames]\n",
    "\n",
    "# get time stampa of each frame\n",
    "selected_timestamps = frame_times[frame_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7c91ca5b-05a0-4554-aa25-8543980ad605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 338.04517,  393.16064,  449.27609,  628.03887, 1141.77774,\n",
       "       1192.52664, 1219.22606, 1264.59175, 1268.67499, 1308.82412,\n",
       "       1405.7887 , 1525.6861 , 1632.33379, 1719.63189, 1777.63062,\n",
       "       1785.56378, 1972.8597 , 2035.625  , 2370.38437, 2416.2167 ,\n",
       "       2439.18287, 2485.58186, 2515.26454, 2535.23078, 2541.34731,\n",
       "       2585.04635, 2667.51122, 2870.49013, 3011.52038, 3200.49959,\n",
       "       3230.84893, 3280.16452, 3305.76396, 3321.71362, 3342.71316,\n",
       "       3435.69446, 3521.72592, 3550.27529, 3578.27468, 3612.95726])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_timestamps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac38779-6678-44be-8c09-81c45620c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get matching side view timestamps\n",
    "side_path = path_file[path_file['full_exp_id'] == experiment_id]['side_timestamp_path']\n",
    "side_time_stamps = np.load(side_path.values[0])\n",
    "\n",
    "\n",
    "side_frame_no = [np.nanargmin(np.abs(side_time_stamps - t)) for t in frame_time_stamps]\n",
    "side_matching_time_stamps = side_time_stamps[side_frame_no]\n",
    "\n",
    "max_time_diff_between_views = np.max(side_matching_time_stamps - frame_time_stamps)\n",
    "assert(max_time_diff_between_views<0.016)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mFish_glm",
   "language": "python",
   "name": "mfish_glm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
